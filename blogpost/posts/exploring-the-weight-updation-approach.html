<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Exploring the weight updation approach</title>
  <meta name="description" content="Recall when you work with neural nets you do update weights after every iteration and while doing so you choose a particular algorithm to update the Parameters associated. So here I have discuused few of well known learning rules.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Exploring the weight updation approach">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://wezn.github.io/blogpost/posts/exploring-the-weight-updation-approach">
  <meta property="og:description" content="Recall when you work with neural nets you do update weights after every iteration and while doing so you choose a particular algorithm to update the Parameters associated. So here I have discuused few of well known learning rules.">
  <meta property="og:site_name" content="ARTICLES">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://wezn.github.io/blogpost/posts/exploring-the-weight-updation-approach">
  <meta name="twitter:title" content="Exploring the weight updation approach">
  <meta name="twitter:description" content="Recall when you work with neural nets you do update weights after every iteration and while doing so you choose a particular algorithm to update the Parameters associated. So here I have discuused few of well known learning rules.">

  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>

  
    <meta property="og:image" content="https://wezn.github.io/blogpost/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
    <meta name="twitter:image" content="https://wezn.github.io/blogpost/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
  

  

  
    <link rel="icon" type="image/x-icon" href="/blogpost/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
    <link rel="apple-touch-icon" href="/blogpost/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
    <link rel="stylesheet" type="text/css" href="/blogpost/assets/light-05326d2f13be8fc447c72fe8d5d8ddf0c01ab921d500806749b40bc63ffd789d.css">
  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        
  <ul class="header-links">
        
      <li>
        <a href="https://wezn.github.io" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/blogpost/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/blogpost/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>
 About me
        </a>
      </li>
    
  </ul>


<nav class="header-nav scrollappear">
  <a href="/blogpost/" class="header-logo" title="ARTICLES">ARTICLES</a>
  <ul class="header-links">
    
    
      <li>
        <a href="https://github.com/VinayIN/" rel="noreferrer noopener" target="_blank" title="GitHub">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-github">
  <use href="/blogpost/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github" xlink:href="/blogpost/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github"></use>
</svg>

        </a>
      </li>
    
    
      <li>
        <a href="https://www.linkedin.com/in/i-binay/" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-linkedin">
  <use href="/blogpost/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin" xlink:href="/blogpost/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin"></use>
</svg>

        </a>
      </li>
    
    
      <li>
        <a href="https://www.facebook.com/binay.pradhan.1996" rel="noreferrer noopener" target="_blank" title="Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-facebook">
  <use href="/blogpost/assets/facebook-72c99b26d37bee65607720abb23d26b44ef30cca1187f98b0d9305be1230b1bf.svg#icon-facebook" xlink:href="/blogpost/assets/facebook-72c99b26d37bee65607720abb23d26b44ef30cca1187f98b0d9305be1230b1bf.svg#icon-facebook"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="https://instagram.com/i_binay/" rel="noreferrer noopener" target="_blank" title="Instagram">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-instagram">
  <use href="/blogpost/assets/instagram-37f85bc75b43ecc4c114d9a46f846327fe13c5893787c6afbcfef9d30d58bd9e.svg#icon-instagram" xlink:href="/blogpost/assets/instagram-37f85bc75b43ecc4c114d9a46f846327fe13c5893787c6afbcfef9d30d58bd9e.svg#icon-instagram"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="mailto:pradhan.binay@outlook.com" title="Email">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-email">
  <use href="/blogpost/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email" xlink:href="/blogpost/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email"></use>
</svg>

        </a>
      </li>
    
    
  </ul>
</nav>

        <article class="article scrollappear">
          <header class="article-header">
            <h1>Exploring the weight updation approach</h1>
            <p>Recall when you work with neural nets you do update weights after every iteration and while doing so you choose a particular algorithm to update the Parameters associated. So here I have discuused few of well known learning rules.</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                August 18, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  6 minute read
                
              </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/blogpost/tag/Deep Learning">Deep Learning</a>
                
                  <a href="/blogpost/tag/Machine Learning">Machine Learning</a>
                
              </div>
            </div>
          </header>

          <div class="article-content">
            <h2 id="exploring-the-weight-updating-approach">Exploring the Weight Updating approach</h2>

<p>Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. Their great success in terms of accuracy is all about training the <code class="highlighter-rouge">net</code> with many samples (i.e- Updating the weight parameters) for every different tasks where its intended to be used. Conventionally, It is stated that <script type="math/tex">W_{ij}</script>= weight connection from <code class="highlighter-rouge">neuron j</code> to <code class="highlighter-rouge">neuron i</code>.</p>

<h3 id="asking-the-next-question-where-did-it-all-start">Asking the next question, Where did it all start?</h3>
<p>The answer says “Updating weight according to architecture”. The world of AI is known by supervised learning, unsupervised learning and reinforcement learning. They employ different learning rules for their weight updation and with varying situation they modify these rule so as to get better results. However their approach mainly starts with these rules:</p>

<ul>
  <li>
    <p><strong>Hebbian learning rule</strong><br />
 Classical Hebb rule indicates “neurons that fire together, wire together”.
 \begin{equation}
 \Delta W = x * Y
 \end{equation}</p>
  </li>
  <li>
    <p><strong>Perceptron learning rule</strong><br />
 update when <script type="math/tex">y \neq Y</script>
 \begin{equation}
 \Delta W = \alpha * Y * x
 \end{equation}</p>
  </li>
  <li>
    <p><strong>Delta learning rule</strong><br />
 calculate error <script type="math/tex">\delta = Y - y</script>
 \begin{equation}
 \Delta W = \alpha * \delta * x
 \end{equation}</p>
  </li>
  <li>
    <p><strong>Backpropagation learning rule</strong><br />
It is also a Generalised Delta Rule, where error is backpropagated from output layer to hidden layer and weight is updated in backward pass.
 \begin{equation}
 \Delta W_{ij} = - \eta \frac {\partial E}{\partial w_{ij}}
 \end{equation}
<u>Simplified representation in MLP</u>,<br />
<script type="math/tex">input_{k} \longrightarrow hidden_{j} \longrightarrow output_{i}</script><br />
<em>for output layer</em>, weights connected to output neuron i<br />
 <script type="math/tex">\frac {\partial E}{\partial w_{ij}} = \delta_{o_i} * out_{h_i}</script><br />
 <script type="math/tex">\delta_{o_i} = (target_{o_i} - out_{o_i}) * out_{o_i}(1-out_{o_i})</script><br />
<em>for hidden layer</em>, weights connected to hidden neuron j<br />
 <script type="math/tex">\frac {\partial E}{\partial w_{jk}} = \delta_{h_j} * input_{k}</script><br />
 <script type="math/tex">\delta_{h_j} = (\sum \delta_{o_i} * w_{h_{j}o_{i}}) * out_{h_j}(1-out_{h_j})</script><br />
Refer to this <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">solved numerical exmaple</a> for a better grasp.</p>
  </li>
  <li>
    <p><strong>Stochastic learning rule</strong>
 This learning is employed in Boltzmann Machine, Cauchy Machine and Hopfield Nets Machine. Stochastic is the random variation included in the network. A popular varation of such learning is Stochatic Gradient Descent (SGD) Rule. A simplified <a href="https://machinelearning-blog.com/2018/02/28/gradient-descent/#more-435">explanation to Gradient Descent optimization algorithm</a> and It is different from Batch (Vanilla) Gradient Descent. <a href="https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1">Read It here</a>. 
 \begin{equation}
 \text {Error/Cost function }\theta(parameter = weights) = \frac {1}{samples} \sum^{samples} (Y - y)^2
 \end{equation}
 This learning rule is to minimise <script type="math/tex">\theta</script>.<br />
 i.e <script type="math/tex">\frac {\partial \theta}{\partial \theta_{\text {(each sample)}}}</script></p>
  </li>
  <li>
    <p><strong>Competitive learning rule</strong>
 This learning approach is refered as “winner-takes-all”. Here during training, the output unit “i” that provides the highest activation to a given input pattern is declared the “weights of the winner” and is moved closer to the input pattern, whereas the rest of the neurons are left unchanged”
 \begin{equation}
 \Delta W_{i} = \eta * x_{(pattern)}
 \end{equation}
 Instar learning rule and Outstar learning rule governs the dynamics of feedback connection weights in a standard competitive neural network in an unsupervised manner. Such learning rules are a part of Kohonen neural Network.</p>
  </li>
</ul>

<p><code class="highlighter-rouge">Reinforcement Learning</code> is a learning to act in order to maximise the future reward. This field has evolved a lot since the use of deep learning.</p>
<div class="embed-responsive embed-responsive-16by9">
  <iframe src="https://www.youtube.com/embed/JgvyzIkgxF0" allowfullscreen=""></iframe>
</div>

<p>Resource to <a href="https://skymind.ai/wiki/deep-reinforcement-learning#reading">explore further</a>.</p>

<h3 id="a-thought-to-explore">A thought to explore:</h3>
<p>Published at <a href="https://iclr.cc">ICLR</a> 2017, Paper titled <a href="https://arxiv.org/abs/1704.04959">“Introspection: Accelerating Neural Network training by learning weight evolution”</a> presents their proposal to allow a neural network to learn the training pattern in a task and utilize it to other similiar tasks. The issue to tackle is the large training time involved that makes the neural nets difficult to use in other various tasks. In this paper, the authors explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.
The conclusion of this paper briefly goes as:</p>

<blockquote>
  <p>A neural network <code class="highlighter-rouge">N</code> is used to learn a general trend in weight evolution of a simple neural network and using it to update weights of many deep neural nets on 3 different tasks - MNIST, CIFAR-10, and ImageNet, with varying network architectures, activations, optimizers, and normalizing strategies. Using the introspection network <code class="highlighter-rouge">N</code> led to faster convergence compared to existing methods in all the cases also utilising small memory footprint.</p>
</blockquote>

<p>The Conclusion and results indicated in the above <code class="highlighter-rouge">titled paper</code> suggests the existence of a general underlying pattern in the weight evolution of any neural network.</p>

<p>Final words “To open a lock, you need <code class="highlighter-rouge">right key</code>. If you allow a locksmith to have a <code class="highlighter-rouge">reference</code>, he will deliver you the key <code class="highlighter-rouge">faster.</code>”. Context to “reference” here is <u>pattern to update weight.</u></p>

          </div>
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Exploring+the+weight+updation+approach%20-%20https://wezn.github.io/blogpost/posts/exploring-the-weight-updation-approach" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://wezn.github.io/blogpost/posts/exploring-the-weight-updation-approach" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
            <a href="https://plus.google.com/share?url=https://wezn.github.io/blogpost/posts/exploring-the-weight-updation-approach" title="Share on Google+" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 128 128"><path d="M40.7 55.9v16.1c0 0 15.6 0 22 0C59.2 82.5 53.8 88.2 40.7 88.2c-13.3 0-23.7-10.8-23.7-24.2s10.4-24.2 23.7-24.2c7.1 0 11.6 2.5 15.8 5.9 3.3-3.3 3.1-3.8 11.6-11.9 -7.2-6.6-16.8-10.6-27.4-10.6C18.2 23.3 0 41.5 0 64c0 22.5 18.2 40.7 40.7 40.7 33.6 0 41.8-29.3 39-48.8H40.7zM113.9 56.7V42.6h-10.1v14.1H89.4v10.1h14.5v14.5h10.1V66.8H128V56.7H113.9z"/></svg>
            </a>
          </div>
<!-- 
           -->
        </article>
        <footer class="footer scrollappear">
  <p>
    Any queries regarding any article is appreciated. Please reach
    <a href="https://wezn.github.io" target="_blank">Binay Pradhan</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  

<script src="/blogpost/assets/vendor-0fb4b91f7ad6c193a69224eba7a01b691a2d7528ee672607575ccc0df3aea545.js" type="text/javascript"></script>


  <script src="/blogpost/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>



  <script src="/blogpost/assets/scrollappear-e2da8ea567e418637e31266cc5302126eaa79f62a2273739086358b589a89ee6.js" type="text/javascript"></script>


<script src="/blogpost/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>

</body>
</html>
